\section{基于P2P网络的信息自主流动机制的方案概述}
本章将阐述基于P2P网络的信息自主流动机制的基本方案。在整套机制中，需要涉及三大研究模块：在文本信息分析与建模的研究中，主要针对互联网中纯文本的信息的非结构化问题，通过数据挖掘和机器学习等方法从中抽取出一部分能够充分表示原文本的特征，并且提出一套适用于各种类型文本的描述方式；在用户兴趣表示与匹配的研究中，主要结合文本信息与用户反馈提出一种具有代表性的兴趣构建方法，从而可以进行用户与用户、用户与资源之间的兴趣相似度计算。在P2P网络路由与发现的研究中，主要利用上述提出的针对资源和用户兴趣的模型，以现在网络为底层架构，构建一层用户兴趣覆盖网络，并提出了网络构建、动态更新与搜索发现等基本方法。最后，基于上述三个模块，本文还设计并实现了一个简易实现的原型系统，主要包括架构设计和功能设计，以此来说明信息自主流动机制的可行性和实际应用价值。

\subsection{文本信息分析与建模}
按照互联网资源的类型可以分为：文本、图片、音乐、视频等。虽然如今涌现了越来越多的诸如音频和视频的流媒体资源，但是互联网上的资源大多任然以文本形式存在，或者可以用文本来代替原有的非文本资源。不仅限于传统的新闻、博客是文本信息，连图片也可以用很成熟的技术来转换成文本，比如百度识图\footnote{http://stu.baidu.com/}就可以自动识别出图片中的物体，即使是视频资源也可以用元信息、标题、评论和弹幕等文本信息进行概括。因此下文主要针对互联网文本信息进行深入研究，并假设其他类型的资源均可以通过已有成熟的方法转换成文本信息。这一小节将根据互联网上四种不同类型的文本，分别提出文本挖掘与建模的方法。

\subsubsection{长文本信息的分析建模方法}
普通的新闻、博客等文章都归类为长文本信息，这类资源一般都是纯文本数据，因此具有两个明显的特征：稀疏性和高维度。举个例子来说：假设给定一由$|\mathcal{V}|$个不同词汇组成的字典$\mathcal{V}$，在由$M$篇文章组成的语料库中，每篇文章$d$的用词都属于词典$\mathcal{V}$，且每篇文章的单词数量不少于$n (0<n\ll W)$。如果直接简单地将文档表示成关于词汇的向量，向量中的每个值表示该词汇在文中的词频，如果该词汇没有在文中出现，则向量中对应的值为0。那么有两点是显而易见的：a）文档向量的维度为$|\mathcal{V}|$，在正常情况下，$|\mathcal{V}|$可以达到百万数量级（$10^6$）；b）文档向量中最多只有$n$个值大于0，一般情况下，$n$只有几百数量级（$10^2$）。这就是纯文本的高维度和稀疏性带来的问题。为了解决这个问题，本文采用基于概率图的主题模型来进行分析，这里先形式化地定义几个重要的基本概念。

对于包含$M$篇文档$d$的语料库$\mathcal{D}$，有$\mathcal{D}=\{d_1,d_2,...,d_M\}$。词汇表$\mathcal{V}$中包含了$W$个不同的词汇$v$。对于文档$d_i$，其中每个单词$w$都取自于$\mathcal{V}$，并且单词可以重复，即$w_i=w_j=v~~(i\ne j, v\in \mathcal{V})$。那么，对于长度为$N$的文档$d$表示为关于单词的向量，$d=\vec{w}=(w_1,w_2,...,w_N)$。

为了简化模型，我们认为文档单词之间是没有先后关系的，换句话说，每个单词相互独立，一个单词出现的概率不会收到另一个单词的影响。因此，我们可以计算文档$d$的生成概率$p(d)$：
\begin{equation}
  p(d)=p(\vec{w})=p(w_1,w_2,...,w_N)=p(w_1)p(w_2)...p(w_N)
\end{equation}
而对于语料库$\mathcal{D}$的生成概率$p(\mathcal{D})$为：
\begin{equation}
  p(\mathcal{D})=p(d_1)p(d_2)...p(d_M)=p(\vec{w_1},\vec{w_2},...,\vec{w_M})
\end{equation}

如果用词频来描述文档的话，$d_m=\vec{n_m}=(n_{m,1},n_{m,2},...,n_{m,W})$，其中$n_{m,i}~~(1\le i\le W)$表示文档$d_m$中出现词汇$v_i$的频数。于是，整个语料库$\mathcal{D}$可以表示为一个$M*W$维的矩阵$X$，即：
\begin{equation*}
  X=\left(
  \begin{array}{cccc}
    n_{1,1} & n_{1,2} & \cdots & n_{1,W} \\
    n_{2,1} & n_{2,2} & \cdots & n_{2,W} \\
    \vdots & \vdots & \ddots & \vdots \\ 
    n_{M,1} & n_{M,2} & \cdots & n_{M,W} \\
  \end{array}
  \right)
\end{equation*}
其中，$n_{m,v}~~(1\le m\le M,1\le v\le W)$表示文档$d$中拥有词汇$v$的数量。

假设词汇$v_i$出现的先验概率为$p_i$，词汇表$\mathcal{V}$中所有词汇组成的先验概率为$\vec{p}=(p_1,p_2,...,p_W)$。又文档$d=\vec{n}=(n_1,n_2,...,n_W)$，那么$d$生成的概率为：
\begin{equation}
  p(d)=p(v_1)^{n_1}p(v_2)^{n_2}...p(v_W)^{n_W}=\prod_{i=1}^{W}p_i^{n_i}
\end{equation}

从整个语料库来看，假设每个词汇$v_i$出现的次数为$n_i$，那么语料库$\mathcal{D}$又可以表示为$\vec{n}=(n_1,n_2,...,n_W)$，在语料库生成过程中，我们可以把$\vec{n}$看做一个是服从多项分布的随机变量，而$(n_1,n_2,...,n_W)$是一组具体的观测值，得到如下：
\begin{equation}
  p(\vec{n})=mult(\vec{n}|\vec{p},N)=
  \begin{pmatrix}
    N \\
    \vec{n}
  \end{pmatrix}
  \sum_{i=1}^Wp_i^{n_i}
\end{equation}

现在的问题是如何利用语料库$\mathcal{D}$已知的数据来估计未知的参数$\vec{p}$？$\vec{p}$的实际意义是词汇表$\mathcal{V}$中每个词汇各自出现的频率。

一种简单的方法是利用最大似然估计（MLE）来估计参数$p_i$的值得到：
\begin{equation}
  \widehat{p_i}=\frac{n_i}{N}
\end{equation}
其中N为语料库的单词总数。这个方法的假设前提是$\vec{p}$的值本身是一个实数常量，换句话说，任何其他语料库$\mathcal{D}'$的生成过程也是依赖于同一个词汇表来生成的。

但是，这个假设对于互联网上的文本是不正确的，有以下三点主要原因：
\begin{itemize}
\item 多类型文档：互联网上的文档类型包括了新闻、技术博客、心情随笔等等，这些文档的用词肯定是很不一样的。比如，对于新闻类型的文档，很少能使用主观色彩的词汇，因此如果将所有新闻文档组成一个语料库的话，主观性形容词的先验概率是十分低的。但是，这与心情随笔类型的文档恰恰相反，因为此类文档大多是抒发作者内心的心情的，如果将此类型文档组成一个语料库的画，主观性形容词的先验概率十分高。
\item 多元化主题：即使是同一类型的文档也有各种不同主题，比如新闻中就包含体育、娱乐、经济、社会、科技等各种主题。对于不同的主题，生成文档的语料库应该也是不同的。如果将上述$\vec{p}$中的值认为是常数的话，相当于所有文档都是从同一个主题中生成的，这显然违背互联网资源的特征。
\item 个性化写作：即使上述两点都保持一致，每个作者写作风格和兴趣爱好同样会影响文档生成的过程。比如说，将每个作者写的所有技术博客各自组成一个语料库，那么这些语料库中的词汇出现概率肯定也千差万别。
\end{itemize}

总而言之，一篇文档的生成过程受到包括文档类型的制约、主题风格的用词和用户个性化的定制等几方面的影响。因此，基于这些原因，对互联网文本的表示不能简单地用一个词频向量$d=\vec{n}=(n_1,n_2,...,n_W)$来表示。

于是，我们考虑到了两个重要的因素：

\subsubsection{短文本信息的挖掘与建模方法}

\subsubsection{关联性文本的挖掘与建模方法}

\subsubsection{动态更新文本的挖掘与建模方法}

\subsection{用户兴趣表示与匹配}

\subsection{P2P网络构建与发现}

\subsection{基于信息自主流动机制的原型系统}

\subsection{小结}
